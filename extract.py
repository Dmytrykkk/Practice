import requests
from bs4 import BeautifulSoup
from newspaper import Article
from urllib.parse import urlparse, urljoin

def get_news_details(url):
    # –û—Ç—Ä–∏–º—É—î–º–æ HTML-–∫–æ–¥ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # –í–∏—Ç—è–≥—É—î–º–æ –¥–æ–º–µ–Ω –∑ URL
    portal_name = urlparse(url).netloc

    # –ü–∞—Ä—Å–∏–º–æ —Å—Ç–∞—Ç—Ç—é
    article = Article(url)
    article.download()
    article.parse()

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title = article.title

    # –û—Å–Ω–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ —É –≤–∏–≥–ª—è–¥—ñ –º–∞—Å–∏–≤—É –∞–±–∑–∞—Ü—ñ–≤
    text_paragraphs = article.text.split("\n")
    text_paragraphs = [para.strip() for para in text_paragraphs if para.strip()]  # –û—á–∏—â–∞—î–º–æ –∞–±–∑–∞—Ü–∏

    # –ü–æ—à—É–∫ –±–ª–æ–∫—É –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
    article_body = soup.find("article") or soup.find("div", {"class": "post-content"}) or soup.find("div", {"class": "content"}) or soup

    # –ü–æ—à—É–∫ –ø–µ—Ä—à–æ–≥–æ –∑–æ–≤–Ω—ñ—à–Ω—å–æ–≥–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ
    source = "–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
    for link in article_body.find_all("a", href=True):
        link_url = urljoin(url, link["href"])  # –û–±—Ä–æ–±–∫–∞ –≤—ñ–¥–Ω–æ—Å–Ω–∏—Ö URL
        parsed_link = urlparse(link_url)

        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —Ç–∞ —Å–æ—Ü–º–µ—Ä–µ–∂—ñ
        if parsed_link.netloc and parsed_link.netloc != portal_name and not any(s in parsed_link.netloc for s in ["t.me", "facebook.com", "twitter.com"]):
            source = link_url  # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à–µ –∑–Ω–∞–π–¥–µ–Ω–µ –∑–æ–≤–Ω—ñ—à–Ω—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è
            break

    return {
        "portal": portal_name,
        "source": source,
        "title": title,
        "text": text_paragraphs  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–µ–∫—Å—Ç —É –≤–∏–≥–ª—è–¥—ñ –º–∞—Å–∏–≤—É –∞–±–∑–∞—Ü—ñ–≤
    }

"""# –¢–µ—Å—Ç
url = "https://website-categorization.whoisxmlapi.com/api"  
news = get_news_details(url)

print(f"üîπ –ü–æ—Ä—Ç–∞–ª: {news['portal']}")
print(f"üîπ –î–∂–µ—Ä–µ–ª–æ: {news['source']}")
print(f"üîπ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {news['title']}")
print("üîπ –¢–µ–∫—Å—Ç:")
for paragraph in news['text']:
    print(paragraph)
"""